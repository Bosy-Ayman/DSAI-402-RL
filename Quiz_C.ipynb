{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObXQ9R6wkWgE8U+fGgwm8U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bosy-Ayman/DSAI-402-RL/blob/main/Quiz_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "sVV1nev3bTaf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Nile Quest - MDP Exam Template\n",
        "==============================\n",
        "This file defines a 6x6 gridworld MDP environment and the required reinforcement learning\n",
        "functions for the lab exam. You are expected to implement the TODOs in this file\n",
        "without using external RL libraries.\n",
        "\n",
        "Rules:\n",
        "- Do not use classes.\n",
        "- You may use numpy only.\n",
        "- Complete the TODO function(s) as described.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Environment Setup\n",
        "# -----------------------------\n",
        "\n",
        "def make_board():\n",
        "    \"\"\"\n",
        "    Create the 6x6 Nile Quest grid.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: 6x6 array of characters representing the environment.\n",
        "                    Legend:\n",
        "                        S - Start\n",
        "                        G - Goal\n",
        "                        H - Hazard (wind)\n",
        "                        Q - Quicksand (absorbing)\n",
        "                        W - Water vortex (double-step)\n",
        "                        R - Rock wall (impassable)\n",
        "                        . - Empty terrain\n",
        "    \"\"\"\n",
        "    return np.array([\n",
        "        list(\"S....G\"),\n",
        "        list(\".R..H.\"),\n",
        "        list(\".Q....\"),\n",
        "        list(\"..W.R.\"),\n",
        "        list(\"..H...\"),\n",
        "        list(\"......\")\n",
        "    ])\n",
        "\n",
        "\n",
        "# Define global constants\n",
        "ACTIONS = ['U', 'R', 'D', 'L']\n",
        "ACTION_DELTA = {\n",
        "    'U': (-1, 0),\n",
        "    'R': (0, 1),\n",
        "    'D': (1, 0),\n",
        "    'L': (0, -1)\n",
        "}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Helper Functions\n",
        "# -----------------------------\n",
        "\n",
        "def in_bounds(state, shape):\n",
        "    \"\"\"\n",
        "    Check whether a state lies inside the grid boundaries.\n",
        "\n",
        "    Args:\n",
        "        state (tuple): (row, column)\n",
        "        shape (tuple): (n_rows, n_cols)\n",
        "\n",
        "    Returns:\n",
        "        bool: True if inside grid, False otherwise.\n",
        "    \"\"\"\n",
        "    r, c = state\n",
        "    nrows, ncols = shape\n",
        "    return 0 <= r < nrows and 0 <= c < ncols\n",
        "\n",
        "\n",
        "def move(state, action, board):\n",
        "    \"\"\"\n",
        "    Compute next position from a given state and action.\n",
        "\n",
        "    Args:\n",
        "        state (tuple): Current position (row, column)\n",
        "        action (str): One of ['U', 'R', 'D', 'L']\n",
        "        board (np.ndarray): Grid environment\n",
        "\n",
        "    Returns:\n",
        "        tuple: Next position (row, column) after applying movement.\n",
        "    \"\"\"\n",
        "    dr, dc = ACTION_DELTA[action]\n",
        "    r, c = state\n",
        "    nr, nc = r + dr, c + dc\n",
        "    if not in_bounds((nr, nc), board.shape) or board[nr, nc] == 'R':\n",
        "        return (r, c)  # stay in place if blocked\n",
        "    return (nr, nc)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Transition Function\n",
        "# -----------------------------\n",
        "\n",
        "def get_transition_probs(state, action, board):\n",
        "    \"\"\"\n",
        "    Get transition probabilities for a given state-action pair.\n",
        "\n",
        "    Args:\n",
        "        state (tuple): Current position (row, column)\n",
        "        action (str): Action taken\n",
        "        board (np.ndarray): Grid environment\n",
        "\n",
        "    Returns:\n",
        "        list[tuple]: List of (probability, next_state) pairs.\n",
        "    \"\"\"\n",
        "    tile = board[state]\n",
        "    probs = []\n",
        "\n",
        "    if tile == 'Q':\n",
        "        # Quicksand is absorbing, but with 2% chance of escape\n",
        "        probs.append((0.98, state))\n",
        "        for a in ACTIONS:\n",
        "            ns = move(state, a, board)\n",
        "            probs.append((0.02 / len(ACTIONS), ns))\n",
        "        return probs\n",
        "\n",
        "    if tile == 'H':\n",
        "        # Hazard: slips left/right 15% each, intended direction 70%\n",
        "        slip_actions = {\n",
        "            'U': ['L', 'R'],\n",
        "            'D': ['R', 'L'],\n",
        "            'L': ['D', 'U'],\n",
        "            'R': ['U', 'D']\n",
        "        }\n",
        "        probs.append((0.7, move(state, action, board)))\n",
        "        probs.append((0.15, move(state, slip_actions[action][0], board)))\n",
        "        probs.append((0.15, move(state, slip_actions[action][1], board)))\n",
        "        return probs\n",
        "\n",
        "    if tile == 'W':\n",
        "        # Water: 50% chance to double-step, else normal step\n",
        "        ns1 = move(state, action, board)\n",
        "        ns2 = move(ns1, action, board)\n",
        "        probs.append((0.5, ns1))\n",
        "        probs.append((0.5, ns2))\n",
        "        return probs\n",
        "\n",
        "    # Default (S, ., G)\n",
        "    probs.append((1.0, move(state, action, board)))\n",
        "    return probs\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Reward Function\n",
        "# -----------------------------\n",
        "\n",
        "def get_reward(state, next_state, board):\n",
        "    \"\"\"\n",
        "    Compute reward for a transition.\n",
        "\n",
        "    Args:\n",
        "        state (tuple): Current state\n",
        "        next_state (tuple): Next state\n",
        "        board (np.ndarray): Grid\n",
        "\n",
        "    Returns:\n",
        "        float: Reward value\n",
        "    \"\"\"\n",
        "    if board[next_state] == 'G':\n",
        "        return 1.0\n",
        "    if board[next_state] == 'Q':\n",
        "        return -0.2\n",
        "    return -0.04  # living cost\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Policy Evaluation\n",
        "# -----------------------------\n",
        "\n",
        "def policy_evaluation(policy, V, board, gamma=0.95, theta=1e-4):\n",
        "    \"\"\"\n",
        "    Evaluate a given policy using iterative policy evaluation.\n",
        "\n",
        "    Args:\n",
        "        policy (dict): Mapping from state -> action.\n",
        "        V (dict): Initial value estimates.\n",
        "        board (np.ndarray): Grid environment.\n",
        "        gamma (float): Discount factor.\n",
        "        theta (float): Convergence threshold.\n",
        "\n",
        "    Returns:\n",
        "        dict: Updated value function after convergence.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for r in range(board.shape[0]):\n",
        "            for c in range(board.shape[1]):\n",
        "                s = (r, c)\n",
        "                if board[s] == 'G':\n",
        "                    continue\n",
        "                v = V[s]\n",
        "                a = policy[s]\n",
        "                new_v = 0\n",
        "                for p, ns in get_transition_probs(s, a, board):\n",
        "                    new_v += p * (get_reward(s, ns, board) + gamma * V[ns])\n",
        "                V[s] = new_v\n",
        "                delta = max(delta, abs(v - new_v))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Policy Improvement\n",
        "# -----------------------------\n",
        "\n",
        "def policy_improvement(V, board, gamma=0.95):\n",
        "    \"\"\"\n",
        "    Improve the policy by acting greedily with respect to V.\n",
        "\n",
        "    Args:\n",
        "        V (dict): Current value function.\n",
        "        board (np.ndarray): Grid environment.\n",
        "        gamma (float): Discount factor.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (new_policy, policy_stable)\n",
        "    \"\"\"\n",
        "    policy = {}\n",
        "    stable = True\n",
        "    for r in range(board.shape[0]):\n",
        "        for c in range(board.shape[1]):\n",
        "            s = (r, c)\n",
        "            if board[s] == 'G':\n",
        "                continue\n",
        "            old_action = policy.get(s, None)\n",
        "            q_values = []\n",
        "            for a in ACTIONS:\n",
        "                q = 0\n",
        "                for p, ns in get_transition_probs(s, a, board):\n",
        "                    q += p * (get_reward(s, ns, board) + gamma * V[ns])\n",
        "                q_values.append(q)\n",
        "            best_a = ACTIONS[np.argmax(q_values)]\n",
        "            policy[s] = best_a\n",
        "            if old_action != best_a:\n",
        "                stable = False\n",
        "    return policy, stable\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Policy Iteration\n",
        "# -----------------------------\n",
        "\n",
        "def policy_iteration(board, gamma=0.95):\n",
        "    \"\"\"\n",
        "    Perform policy iteration to find optimal policy and value function.\n",
        "\n",
        "    Args:\n",
        "        board (np.ndarray): Grid.\n",
        "        gamma (float): Discount factor.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (optimal_policy, optimal_value_function)\n",
        "    \"\"\"\n",
        "    policy = {(r, c): np.random.choice(ACTIONS) for r in range(board.shape[0]) for c in range(board.shape[1])}\n",
        "    V = {(r, c): 0 for r in range(board.shape[0]) for c in range(board.shape[1])}\n",
        "\n",
        "    while True:\n",
        "        V = policy_evaluation(policy, V, board, gamma)\n",
        "        policy, stable = policy_improvement(V, board, gamma)\n",
        "        if stable:\n",
        "            break\n",
        "    return policy, V\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Value Iteration (TODO)\n",
        "# -----------------------------\n",
        "\n",
        "def value_iteration(board, gamma=0.95, theta=1e-4):\n",
        "    \"\"\"\n",
        "    Perform value iteration to compute the optimal value function and policy.\n",
        "\n",
        "    Args:\n",
        "        board (np.ndarray): Grid environment.\n",
        "        gamma (float): Discount factor.\n",
        "        theta (float): Convergence threshold.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (optimal_value_function, optimal_policy)\n",
        "\n",
        "    TODO:\n",
        "        Implement this function by:\n",
        "        1. Initializing all V(s) = 0.\n",
        "        2. Iteratively updating V(s) using the Bellman optimality equation:\n",
        "               V(s) <- max_a Σ [ P(s'|s,a) * (R(s,a,s') + γ * V(s')) ]\n",
        "        3. Repeat until the change in V(s) < θ.\n",
        "        4. Derive the optimal policy π*(s) from the final V(s).\n",
        "    \"\"\"\n",
        "    V = {(r, c): 0 for r in range(board.shape[0]) for c in range(board.shape[1])}\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for r in range(board.shape[0]):\n",
        "            for c in range(board.shape[1]):\n",
        "                s = (r, c)\n",
        "                if board[s] == 'G':\n",
        "                    continue\n",
        "                v = V[s]\n",
        "                q_values = []\n",
        "                q = 0\n",
        "                for a in ACTIONS:\n",
        "                    for p, ns in get_transition_probs(s, a, board):\n",
        "                        q += p * (get_reward(s, ns, board) + gamma * V[ns])\n",
        "                    q_values.append(q)\n",
        "\n",
        "                V[s] = max(q_values)\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "        # value optmization\n",
        "        policy = {}\n",
        "        for r in range(board.shape[0]):\n",
        "            for c in range(board.shape[1]):\n",
        "                s = (r, c)\n",
        "                if board[s] == 'G':\n",
        "                    continue\n",
        "                q_values = []\n",
        "                for a in ACTIONS:\n",
        "                    q = 0\n",
        "                    for p, ns in get_transition_probs(s, a, board):\n",
        "                        q += p * (get_reward(s, ns, board) + gamma * V[ns])\n",
        "                    q_values.append(q)\n",
        "                best_a = ACTIONS[np.argmax(q_values)]\n",
        "                policy[s] = best_a\n",
        "\n",
        "    return V,policy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Simulation Functions\n",
        "# -----------------------------\n",
        "\n",
        "def simulate_episode(policy, board, start=(0, 0), gamma=0.95, max_steps=100):\n",
        "    \"\"\"\n",
        "    Simulate one episode following the given policy.\n",
        "\n",
        "    Args:\n",
        "        policy (dict): Deterministic policy mapping state -> action.\n",
        "        board (np.ndarray): Environment grid.\n",
        "        start (tuple): Starting state coordinates.\n",
        "        gamma (float): Discount factor.\n",
        "        max_steps (int): Maximum episode length.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (total_reward, steps)\n",
        "               total_reward (float): Discounted cumulative return.\n",
        "               steps (int): Number of steps taken.\n",
        "    \"\"\"\n",
        "    s = start\n",
        "    total_reward, step = 0, 0\n",
        "\n",
        "    for t in range(max_steps):\n",
        "        if board[s] == 'G':\n",
        "            break\n",
        "        a = policy.get(s, np.random.choice(ACTIONS))\n",
        "        transitions = get_transition_probs(s, a, board)\n",
        "        probs, next_states = zip(*transitions)\n",
        "        s_next = next_states[np.random.choice(len(next_states), p=probs)]\n",
        "        r = get_reward(s, s_next, board)\n",
        "        total_reward += (gamma ** t) * r\n",
        "        s = s_next\n",
        "        step += 1\n",
        "        if board[s] == 'G':\n",
        "            break\n",
        "    return total_reward, step\n",
        "\n",
        "\n",
        "def run_experiments(policy, board, episodes=50):\n",
        "    \"\"\"\n",
        "    Run multiple simulations and report average results.\n",
        "\n",
        "    Args:\n",
        "        policy (dict): Policy to evaluate.\n",
        "        board (np.ndarray): Environment grid.\n",
        "        episodes (int): Number of episodes to simulate.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (avg_reward, avg_steps)\n",
        "    \"\"\"\n",
        "    rewards, steps = [], []\n",
        "    for _ in range(episodes):\n",
        "        r, s = simulate_episode(policy, board)\n",
        "        rewards.append(r)\n",
        "        steps.append(s)\n",
        "    return np.mean(rewards), np.mean(steps)"
      ],
      "metadata": {
        "id": "uc3gr3jviBrY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the full MDP simulation once value_iteration() is implemented\n",
        "\n",
        "board = make_board()\n",
        "gamma = 0.95\n",
        "V, policy = value_iteration(board, gamma)  # <-- you must have implement this\n",
        "\n",
        "avg_return, avg_steps = run_experiments(policy, board, episodes=100)\n",
        "print(f\"\\nAverage discounted return: {avg_return:.4f}\")\n",
        "print(f\"Average steps to goal: {avg_steps:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR22bH8GiD3U",
        "outputId": "9d0d8cee-7a8a-4c5b-f348-0cfa6ec5834f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average discounted return: -0.7953\n",
            "Average steps to goal: 100.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "cUQkaP-3oM_y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2JyEXLpKoFZH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}