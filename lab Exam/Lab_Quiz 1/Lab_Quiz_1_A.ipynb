{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mJVJ15PuDnJ"
      },
      "source": [
        "#yourNAME, yourID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVmskFXtt9ck"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qLc35GaOt8PX"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3tcl2yet_iq"
      },
      "source": [
        "# LuffyDodge (3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CV2uRUPNoNLy"
      },
      "outputs": [],
      "source": [
        "class LuffyDodgeEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom environment where Luffy must dodge cannonballs.\n",
        "    Luffy can move left, right, or stay still to avoid being hit.\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\"]}\n",
        "\n",
        "    def __init__(self, render_mode=None):\n",
        "        \"\"\"\n",
        "        Initialize environment parameters.\n",
        "        \"\"\"\n",
        "        super(LuffyDodgeEnv, self).__init__()\n",
        "\n",
        "        # Actions: 0 = left, 1 = stay, 2 = right\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Observation: [Luffy_x, Cannonball_x, Cannonball_y]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, 0], dtype=np.float32),\n",
        "            high=np.array([9, 9, 9], dtype=np.float32),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to its initial state.\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Luffy starts in the middle bottom\n",
        "        self.luffy_x = 5.0\n",
        "\n",
        "        # Cannonball starts at a random x and top y = 9\n",
        "        self.cannon_x = np.random.randint(0, 10)\n",
        "        self.cannon_y = 9.0\n",
        "\n",
        "        # Return initial observation\n",
        "        obs = np.array([self.luffy_x, self.cannon_x, self.cannon_y], dtype=np.float32)\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Performs one time-step transition in the environment.\n",
        "\n",
        "        Parameters:\n",
        "            action (int): The action taken by Luffy.\n",
        "                          0 = move left, 1 = stay still, 2 = move right\n",
        "\n",
        "        Returns:\n",
        "            observation (np.array): The next state [luffy_x, cannon_x, cannon_y]\n",
        "            reward (float): +1 if survived this step, -10 if hit by cannonball\n",
        "            done (bool): True if Luffy is hit, False otherwise\n",
        "            truncated (bool): Optional flag if max steps are reached\n",
        "            info (dict): Additional info (empty for now)\n",
        "\n",
        "        Notes for Students:\n",
        "        - Update Luffy’s position based on the action.\n",
        "        - Move the cannonball down by one step.\n",
        "        - Check for collision (same x and y == 0).\n",
        "        - If collision occurs, assign -10 reward and mark `done = True`.\n",
        "        - Otherwise, assign +1 reward and continue.\n",
        "        - If the cannonball reaches y = 0 but misses Luffy, reset it to top at a random x position.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        Render the environment as simple text output.\n",
        "        \"\"\"\n",
        "        grid = np.full((10, 10), \" \", dtype=str)\n",
        "        grid[int(self.cannon_y), int(self.cannon_x)] = \"O\"\n",
        "        grid[0, int(self.luffy_x)] = \"L\"\n",
        "        print(\"\\n\".join([\"\".join(row) for row in grid[::-1]]))\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the environment.\"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px1liZDDt0Tb"
      },
      "outputs": [],
      "source": [
        "env = LuffyDodgeEnv()\n",
        "obs, _ = env.reset()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, truncated, info = env.step(action)\n",
        "    env.render()\n",
        "    print(f\"Action: {action}, Reward: {reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbz9B-T0t3lu"
      },
      "source": [
        "# Policy Evaluation (2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qV9XOHIpzUn"
      },
      "outputs": [],
      "source": [
        "# Grid size\n",
        "GRID_SIZE = 5\n",
        "\n",
        "# Actions: 0 = left, 1 = stay, 2 = right\n",
        "ACTIONS = [0, 1, 2]\n",
        "NUM_ACTIONS = len(ACTIONS)\n",
        "\n",
        "# All possible states: (Luffy_x, Cannon_x, Cannon_y)\n",
        "states = [(lx, cx, cy) for lx in range(GRID_SIZE)\n",
        "                         for cx in range(GRID_SIZE)\n",
        "                         for cy in range(GRID_SIZE)]\n",
        "NUM_STATES = len(states)\n",
        "\n",
        "# Initialize policy randomly\n",
        "policy = np.random.choice(ACTIONS, size=NUM_STATES)\n",
        "\n",
        "# Initialize value function\n",
        "V = np.zeros(NUM_STATES)\n",
        "\n",
        "# Discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "\n",
        "def state_to_index(state):\n",
        "    \"\"\"Convert a (luffy_x, cannon_x, cannon_y) state tuple into its index in the state list.\"\"\"\n",
        "    lx, cx, cy = state\n",
        "    return lx * GRID_SIZE * GRID_SIZE + cx * GRID_SIZE + cy\n",
        "\n",
        "\n",
        "def transition(state, action):\n",
        "    \"\"\"\n",
        "    Deterministic transition function for the environment.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    state : tuple (luffy_x, cannon_x, cannon_y)\n",
        "    action : int (0=left, 1=stay, 2=right)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    next_state : tuple or None\n",
        "        Next state after taking the action.\n",
        "    reward : float\n",
        "        Reward received after the transition.\n",
        "    done : bool\n",
        "        Whether the episode terminates (Luffy hit by cannonball).\n",
        "    \"\"\"\n",
        "    lx, cx, cy = state\n",
        "\n",
        "    # Move Luffy\n",
        "    if action == 0:\n",
        "        lx = max(0, lx - 1)\n",
        "    elif action == 2:\n",
        "        lx = min(GRID_SIZE - 1, lx + 1)\n",
        "\n",
        "    # Move cannonball down\n",
        "    cy -= 1\n",
        "\n",
        "    # Check terminal condition\n",
        "    if cy < 0:\n",
        "        if lx == cx:\n",
        "            # Hit\n",
        "            return None, -10.0, True\n",
        "        else:\n",
        "            # Miss → reset cannonball to top\n",
        "            return (lx, cx, GRID_SIZE - 1), +1.0, False\n",
        "\n",
        "    # Otherwise just one step closer to bottom\n",
        "    return (lx, cx, cy), +1.0, False\n",
        "\n",
        "\n",
        "def policy_evaluation(policy, V, theta=1e-4):\n",
        "    \"\"\"\n",
        "    TODO: Students must implement this function.\n",
        "\n",
        "    This function should perform **policy evaluation** — i.e.,\n",
        "    iteratively compute the state-value function V(s) for the\n",
        "    current policy π until convergence.\n",
        "\n",
        "    What to do:\n",
        "    -----------\n",
        "    1. Repeat until the value function changes very little (Δ < θ):\n",
        "       - For each state s:\n",
        "           * Get the action `a = policy[s]`\n",
        "           * Use `transition(state, a)` to get (next_state, reward, done)\n",
        "           * If done: V[s] = reward\n",
        "           * Else:   V[s] = reward + γ * V[next_state_index]\n",
        "\n",
        "    2. Stop when the maximum change in any V[s] is below `theta`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    policy : np.array\n",
        "        Current policy mapping each state index to an action (0,1,2).\n",
        "    V : np.array\n",
        "        Current value estimates for each state.\n",
        "    theta : float\n",
        "        Convergence threshold for stopping condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    V : np.array\n",
        "        Updated state-value function for the given policy.\n",
        "    \"\"\"\n",
        "    # YOU should fill in the implementation here\n",
        "    pass\n",
        "\n",
        "\n",
        "def policy_improvement(V, policy):\n",
        "    \"\"\"Greedy policy improvement based on the current value function.\"\"\"\n",
        "    policy_stable = True\n",
        "    for s, state in enumerate(states):\n",
        "        old_action = policy[s]\n",
        "        action_values = []\n",
        "\n",
        "        # Try all actions and pick the best one\n",
        "        for a in ACTIONS:\n",
        "            next_state, reward, done = transition(state, a)\n",
        "            if done:\n",
        "                action_values.append(reward)\n",
        "            else:\n",
        "                action_values.append(reward + gamma * V[state_to_index(next_state)])\n",
        "\n",
        "        best_action = np.argmax(action_values)\n",
        "        policy[s] = best_action\n",
        "\n",
        "        # Check if policy changed\n",
        "        if old_action != best_action:\n",
        "            policy_stable = False\n",
        "    return policy, policy_stable\n",
        "\n",
        "\n",
        "def policy_iteration():\n",
        "    \"\"\"Run the full policy iteration loop.\"\"\"\n",
        "    global V, policy\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        V = policy_evaluation(policy, V)\n",
        "        policy, stable = policy_improvement(V, policy)\n",
        "        print(f\"Iteration {iteration} completed.\")\n",
        "        if stable:\n",
        "            print(\"✅ Policy converged!\")\n",
        "            break\n",
        "    return policy, V\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    optimal_policy, optimal_V = policy_iteration()\n",
        "\n",
        "    print(\"\\nOptimal policy (sample of 10 states):\")\n",
        "    for i in range(10):\n",
        "        print(f\"State {states[i]} → Action {optimal_policy[i]}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cbz9B-T0t3lu"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
