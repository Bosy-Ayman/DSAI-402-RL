{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jY5NHGuAlMZQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "a9960ca1-14db-4f87-b308-2dcc19cf763d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mars Terrain:\n",
            "[['S' '.' '.' '.' '.' 'B']\n",
            " ['.' 'R' '.' '.' 'D' '.']\n",
            " ['.' 'C' '.' '.' '.' '.']\n",
            " ['.' '.' 'T' '.' 'R' '.']\n",
            " ['.' '.' 'D' '.' '.' '.']\n",
            " ['.' '.' '.' '.' '.' '.']]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "tuple indices must be integers or slices, not tuple",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1825853232.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1825853232.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;31m# Once implemented, you can test either:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0;31m# or:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;31m# V, policy = value_iteration(board)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1825853232.py\u001b[0m in \u001b[0;36mpolicy_iteration\u001b[0;34m(board, gamma)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_improvement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1825853232.py\u001b[0m in \u001b[0;36mpolicy_improvement\u001b[0;34m(V, board, gamma)\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_transition_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                     \u001b[0mq\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mbest_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Mars Rover - MDP Exam Template\n",
        "==============================\n",
        "You are designing a control system for an autonomous Mars Rover exploring a 6x6 grid region.\n",
        "\n",
        "Each grid cell has a specific terrain type that affects movement and rewards.\n",
        "\n",
        "Your goal:\n",
        "Implement the missing Reinforcement Learning functions to help the rover find the best exploration strategy.\n",
        "\n",
        "Rules:\n",
        "- Do not use external RL libraries.\n",
        "- You may use numpy only.\n",
        "- Complete ALL TODO functions as described below.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Environment Setup\n",
        "# -----------------------------\n",
        "\n",
        "def make_board():\n",
        "    \"\"\"\n",
        "    Create the 6x6 Mars terrain grid.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: 6x6 array representing terrain types.\n",
        "                    Legend:\n",
        "                        S - Start (landing site)\n",
        "                        B - Base (target location)\n",
        "                        D - Dust storm (slippery)\n",
        "                        C - Crater (absorbing)\n",
        "                        T - Tunnel (double-step)\n",
        "                        R - Rock wall (impassable)\n",
        "                        . - Flat ground\n",
        "    \"\"\"\n",
        "    return np.array([\n",
        "        list(\"S....B\"),\n",
        "        list(\".R..D.\"),\n",
        "        list(\".C....\"),\n",
        "        list(\"..T.R.\"),\n",
        "        list(\"..D...\"),\n",
        "        list(\"......\")\n",
        "    ])\n",
        "\n",
        "\n",
        "# Define constants\n",
        "ACTIONS = ['U', 'R', 'D', 'L']\n",
        "ACTION_DELTA = {\n",
        "    'U': (-1, 0),\n",
        "    'R': (0, 1),\n",
        "    'D': (1, 0),\n",
        "    'L': (0, -1)\n",
        "}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Helper Functions\n",
        "# -----------------------------\n",
        "\n",
        "def in_bounds(state, shape):\n",
        "    \"\"\"Check whether a state lies inside the grid boundaries.\"\"\"\n",
        "    r, c = state\n",
        "    nrows, ncols = shape\n",
        "    return 0 <= r < nrows and 0 <= c < ncols\n",
        "\n",
        "\n",
        "def move(state, action, board):\n",
        "    \"\"\"Compute next position from a given state and action.\"\"\"\n",
        "    dr, dc = ACTION_DELTA[action]\n",
        "    r, c = state\n",
        "    nr, nc = r + dr, c + dc\n",
        "    if not in_bounds((nr, nc), board.shape) or board[nr, nc] == 'R':\n",
        "        return (r, c)  # blocked by rocks\n",
        "    return (nr, nc)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Transition Function\n",
        "# -----------------------------\n",
        "\n",
        "def get_transition_probs(state, action, board):\n",
        "    \"\"\"\n",
        "    Get transition probabilities for a given state-action pair.\n",
        "    \"\"\"\n",
        "    tile = board[state]\n",
        "    probs = []\n",
        "\n",
        "    if tile == 'C':\n",
        "        # Crater: absorbing, but 2% chance of random ejection\n",
        "        probs.append((0.98, state))\n",
        "        for a in ACTIONS:\n",
        "            ns = move(state, a, board)\n",
        "            probs.append((0.02 / len(ACTIONS), ns))\n",
        "        return probs\n",
        "\n",
        "    if tile == 'D':\n",
        "        # Dust storm: slips sideways 15% each, intended direction 70%\n",
        "        slip_actions = {\n",
        "            'U': ['L', 'R'],\n",
        "            'D': ['R', 'L'],\n",
        "            'L': ['D', 'U'],\n",
        "            'R': ['U', 'D']\n",
        "        }\n",
        "        probs.append((0.7, move(state, action, board)))\n",
        "        probs.append((0.15, move(state, slip_actions[action][0], board)))\n",
        "        probs.append((0.15, move(state, slip_actions[action][1], board)))\n",
        "        return probs\n",
        "\n",
        "    if tile == 'T':\n",
        "        # Tunnel: 50% chance to move one step, 50% to double-step\n",
        "        ns1 = move(state, action, board)\n",
        "        ns2 = move(ns1, action, board)\n",
        "        probs.append((0.5, ns1))\n",
        "        probs.append((0.5, ns2))\n",
        "        return probs\n",
        "\n",
        "    # Default terrain\n",
        "    probs.append((1.0, move(state, action, board)))\n",
        "    return probs\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Reward Function\n",
        "# -----------------------------\n",
        "\n",
        "def get_reward(state, next_state, board):\n",
        "    \"\"\"Compute reward for a transition.\"\"\"\n",
        "    if board[next_state] == 'B':\n",
        "        return 1.0\n",
        "    if board[next_state] == 'C':\n",
        "        return -0.2\n",
        "    return -0.04  # energy cost\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Policy Evaluation (TODO)\n",
        "# -----------------------------\n",
        "\n",
        "def policy_evaluation(policy, V, board, gamma=0.95, theta=1e-4):\n",
        "    \"\"\"\n",
        "    Evaluate a given policy using iterative policy evaluation.\n",
        "\n",
        "    Args:\n",
        "        policy (dict): Mapping from state -> action.\n",
        "        V (dict): Initial value estimates.\n",
        "        board (np.ndarray): Grid environment.\n",
        "        gamma (float): Discount factor.\n",
        "        theta (float): Convergence threshold.\n",
        "\n",
        "    Returns:\n",
        "        dict: Updated value function after convergence.\n",
        "\n",
        "    TODO:\n",
        "        Implement this function by:\n",
        "        1. Iteratively updating V(s) using:\n",
        "               V(s) ← Σ [ P(s'|s,π(s)) * (R(s,π(s),s') + γ * V(s')) ]\n",
        "        2. Repeat until max change in V(s) < θ.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "      delta = 0\n",
        "      for r in range(board.shape[0]):\n",
        "          for c in range(board.shape[1]):\n",
        "              s = (r, c)\n",
        "              if board[s] == 'B':\n",
        "                  continue\n",
        "\n",
        "              old_action = policy.get(s, None)\n",
        "              v = V[s]\n",
        "              for p, ns in get_transition_probs(s,old_action, board):\n",
        "                    V[s] += p * (get_reward(s, ns, board) + gamma * V[ns])\n",
        "              # policy[s] = v\n",
        "              delta = max(delta,abs(v-V[s]))\n",
        "\n",
        "      if delta < theta:\n",
        "        break\n",
        "    return policy, V\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Policy Improvement\n",
        "# -----------------------------\n",
        "\n",
        "def policy_improvement(V, board, gamma=0.95):\n",
        "    \"\"\"Improve policy greedily based on current value estimates.\"\"\"\n",
        "    policy = {}\n",
        "    stable = True\n",
        "    for r in range(board.shape[0]):\n",
        "        for c in range(board.shape[1]):\n",
        "            s = (r, c)\n",
        "            if board[s] == 'B':\n",
        "                continue\n",
        "            old_action = policy.get(s, None)\n",
        "\n",
        "            q_values = []\n",
        "            for a in ACTIONS:\n",
        "                q = 0\n",
        "                for p, ns in get_transition_probs(s, a, board):\n",
        "\n",
        "                    q += p * (get_reward(s, ns, board) + gamma * V[ns])\n",
        "                q_values.append(q)\n",
        "            best_a = ACTIONS[np.argmax(q_values)]\n",
        "            policy[s] = best_a\n",
        "            if old_action != best_a:\n",
        "                stable = False\n",
        "    return policy, stable\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Policy Iteration\n",
        "# -----------------------------\n",
        "\n",
        "def policy_iteration(board, gamma=0.95):\n",
        "    \"\"\"Perform policy iteration to find optimal policy and values.\"\"\"\n",
        "    policy = {(r, c): np.random.choice(ACTIONS) for r in range(board.shape[0]) for c in range(board.shape[1])}\n",
        "    V = {(r, c): 0 for r in range(board.shape[0]) for c in range(board.shape[1])}\n",
        "\n",
        "    while True:\n",
        "        V = policy_evaluation(policy, V, board, gamma)\n",
        "        policy, stable = policy_improvement(V, board, gamma)\n",
        "        if stable:\n",
        "            break\n",
        "    return policy, V\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Value Iteration (TODO)\n",
        "# -----------------------------\n",
        "\n",
        "def value_iteration(board, gamma=0.95, theta=1e-4):\n",
        "    \"\"\"\n",
        "    Perform value iteration to compute the optimal value function and policy.\n",
        "\n",
        "    Args:\n",
        "        board (np.ndarray): Grid environment.\n",
        "        gamma (float): Discount factor.\n",
        "        theta (float): Convergence threshold.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (optimal_value_function, optimal_policy)\n",
        "\n",
        "    TODO:\n",
        "        Implement this function by:\n",
        "        1. Initialize all V(s) = 0.\n",
        "        2. Iteratively update each V(s) using:\n",
        "               V(s) ← max_a Σ [ P(s'|s,a) * (R(s,a,s') + γ * V(s')) ]\n",
        "        3. Stop when the max change in V(s) < θ.\n",
        "        4. From the final V(s), derive the optimal policy π*(s).\n",
        "    \"\"\"\n",
        "    V = {(r, c): 0 for r in range(board.shape[0]) for c in range(board.shape[1])}\n",
        "    while True:\n",
        "      delta = 0\n",
        "      for r in range(board.shape[0]):\n",
        "          for c in range(board.shape[1]):\n",
        "              s = (r, c)\n",
        "              if board[s] == 'B':\n",
        "                  continue\n",
        "\n",
        "              # old_action = policy.get(s, None)\n",
        "              q_values = []\n",
        "              v = V[s]\n",
        "              for a in ACTIONS:\n",
        "                q=0\n",
        "                for p, ns in get_transition_probs(s, a, board):\n",
        "                      q += p * (get_reward(s, ns, board) + gamma * V[ns])\n",
        "                q_values.append(q)\n",
        "              best_a = max(q_values)\n",
        "              delta = max(delta,abs(v-V[s]))\n",
        "              best_a = max(q_values)\n",
        "              V[s]= best_a\n",
        "          if delta < theta:\n",
        "               break\n",
        "\n",
        "      # Optimal Value\n",
        "      policy = {}\n",
        "      for r in range(board.shape[0]):\n",
        "            for c in range(board.shape[1]):\n",
        "                s = (r, c)\n",
        "                if board[s] == 'B':\n",
        "                  continue\n",
        "                old_action = policy.get(s, None)\n",
        "                q_values = []\n",
        "                for a in ACTIONS:\n",
        "                  q = 0\n",
        "                  for p, ns in get_transition_probs(s, a, board):\n",
        "                         q += p * (get_reward(s, ns, board) + gamma * V[ns])\n",
        "                  q_values.append(q)\n",
        "                  best_a = ACTIONS[np.argmax(q_values)]\n",
        "                  policy[s] = best_a\n",
        "\n",
        "    return policy, V\n",
        "\n",
        "# -----------------------------\n",
        "# Experiment Runner\n",
        "# -----------------------------\n",
        "\n",
        "def run_experiment():\n",
        "    \"\"\"Utility to run and test algorithms after implementation.\"\"\"\n",
        "    board = make_board()\n",
        "    print(\"Mars Terrain:\")\n",
        "    print(board)\n",
        "\n",
        "    # Once implemented, you can test either:\n",
        "    policy, V = policy_iteration(board)\n",
        "    # or:\n",
        "    # V, policy = value_iteration(board)\n",
        "    # and visualize or print results\n",
        "    print(\"\\nImplement the TODO functions and test them here.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_experiment()\n"
      ]
    }
  ]
}