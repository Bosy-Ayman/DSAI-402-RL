Itâ€™s a classic environment from **OpenAI Gym / Gymnasium**.  
The agent moves on a frozen lake made of tiles:  
some are **frozen (safe)** and some are **holes (fall = lose)**.

Goal â†’ start at `S`, reach goal `G`, avoid holes `H`

![[Pasted image 20251031140735.png]]
## ğŸ§© What kind of Environment is it?

### 1ï¸âƒ£ **Environment type: Stochastic**

âœ… **Itâ€™s stochastic (non-deterministic).**

- Even if you choose an action (like "RIGHT"),  
    the agent **might slip** and move in another direction instead.

For example:

- Action = â€œRIGHTâ€
    
- Actual move:
    
    - 80% â†’ RIGHT
        
    - 10% â†’ UP
        
    - 10% â†’ DOWN
        

That means the **same action in the same state** can lead to **different next states**.

So itâ€™s **a stochastic environment**.

### 2ï¸âƒ£ **Policy type: You choose**

The **policy** depends on your algorithm.

- If you use **deterministic policy**, it always picks one best action per state.
    
- If you use **stochastic policy** (e.g., during exploration), it picks actions randomly with certain probabilities.

So the **environment** itself is **stochastic**,  
but the **policy** can be **deterministic or stochastic** â€” depending on what youâ€™re using.
### 3ï¸âƒ£ **Observation & Action Spaces**

| Element          | Description                                   |
| ---------------- | --------------------------------------------- |
| **State space**  | Discrete â€” each tile is a state.              |
| **Action space** | Discrete â€” 4 actions (LEFT, DOWN, RIGHT, UP). |
| **Rewards**      | +1 when reaching the goal, 0 otherwise.       |
## ğŸ§  Summary

| Property     | Type                                   | Description                                         |
| ------------ | -------------------------------------- | --------------------------------------------------- |
| Environment  | **Stochastic**                         | Slippery â€” actions donâ€™t always go where you expect |
| State space  | **Discrete**                           | Each tile on the grid                               |
| Action space | **Discrete**                           | 4 directions                                        |
| Policy       | **Can be deterministic or stochastic** | You decide based on the algorithm                   |
we can actually **make FrozenLake deterministic** by setting:

```python

import gymnasium as gym env = gym.make("FrozenLake-v1", is_slippery=False)
```


Then it becomes **deterministic**:

- â€œRIGHTâ€ always moves right.
    
- â€œDOWNâ€ always moves down.

So you can switch between:

- `is_slippery=True` â†’ **stochastic**

- `is_slippery=False` â†’ **deterministic**
---

### Needed Libraries:
Numpy - gymnasium 

```python
  pip install "gymnasium[toy-text]"
  pip install numpy
  pip install gymnasium

```

## Imports
```python
import numpy as np

import gymnasium as gym Â 

```

## Environment

```Python

env = gym.make("FrozenLake-v1", is_slippery=False, render_mode="human")

```


---
# [1] Policy Iteration

```python

class PolicyIteration:

Â  def __init__(self, env, discount_factor=0.9, theta=1e-6, max_iterations=1000):

Â  Â  self.env = env

Â  Â  self.discount_factor = discount_factor

Â  Â  self.theta = theta Â  Â  Â  Â  Â  Â  Â  Â  Â  # small threshold for stopping policy evaluation

Â  Â  self.max_iterations = max_iterations

Â  Â  self.num_states = env.observation_space.n

Â  Â  self.num_actions = env.action_space.n

  

Â  Â  # initialize policy and value table

Â  Â  self.policy = np.zeros(self.num_states, dtype=int) Â # all actions = 0 initially

Â  Â  self.value_table = np.zeros(self.num_states) Â  Â  Â  Â  # start with all zeros

  

Â  # 2ï¸âƒ£ Policy Evaluation

Â  def policy_evaluation(self):

Â  Â  for i in range(self.max_iterations):

Â  Â  Â  delta = 0 Â # check change amount to know when to stop

Â  Â  Â  for state in range(self.num_states):

Â  Â  Â  Â  action = self.policy[state]

Â  Â  Â  Â  value = 0

Â  Â  Â  Â  for prob, next_state, reward, done in self.env.P[state][action]:

Â  Â  Â  Â  Â  value += prob * (reward + self.discount_factor * self.value_table[next_state])

Â  Â  Â  Â  delta = max(delta, abs(self.value_table[state] - value))

Â  Â  Â  Â  self.value_table[state] = value

Â  Â  Â  if delta < self.theta:

Â  Â  Â  Â  break


Â  # 3ï¸âƒ£ Policy Improvement

Â  def policy_improvement(self):

Â  Â  policy_stable = True

Â  Â  for state in range(self.num_states):

Â  Â  Â  old_action = self.policy[state]

Â  Â  Â  action_Q = np.zeros(self.num_actions)

  

Â  Â  Â  # compute Q(s,a) for each action

Â  Â  Â  for action in range(self.num_actions):

Â  Â  Â  Â  for prob, next_state, reward, done in self.env.P[state][action]:

Â  Â  Â  Â  Â  action_Q[action] += prob * (reward + self.discount_factor * self.value_table[next_state])

  

Â  Â  Â  # choose best action

Â  Â  Â  best_action = np.argmax(action_Q)

Â  Â  Â  self.policy[state] = best_action

  

Â  Â  Â  if old_action != best_action:

Â  Â  Â  Â  policy_stable = False

  

Â  Â  return policy_stable

  

```



```python

Â  def run(self):
Â  Â  iteration = 0

Â  Â  while True:

Â  Â  Â  iteration += 1

Â  Â  Â  self.policy_evaluation()

Â  Â  Â  stable = self.policy_improvement()

Â  Â  Â  if stable:

Â  Â  Â  Â  print(f"âœ… Policy converged after {iteration} iterations")

Â  Â  Â  Â  break

  

Â  Â  print("\nâœ… Final Value Table:")

Â  Â  print(self.value_table)

Â  Â  print("\nâœ… Optimal Policy:")

Â  Â  print(self.policy)

Â  Â  return self.policy


```

# 6ï¸âƒ£ Apply Policy Iteration

```python


agent = PolicyIteration(env.unwrapped, discount_factor=0.9)

optimal_policy = agent.run()


# 7ï¸âƒ£ Test the learned policy


state, info = env.reset()

done = False

path = [state]

  

print("\nğŸ¯ Following Optimal Policy:\n")

while not done:

Â  action = int(optimal_policy[state])

Â  next_state, reward, terminated, truncated, info = env.step(action)

Â  done = terminated or truncated

Â  path.append(next_state)

Â  env.render()

Â  state = next_state

  

print("\nğŸ Path of visited states:")

print(path)

```


# [2] Value Iteration


```python

class ValueIteration:

Â  def __init__(self, env, n_iteration, discount_factor):

Â  Â  self.env = env

Â  Â  self.n_iteration = n_iteration

Â  Â  self.discount_factor = discount_factor

Â  Â  self.num_states = env.observation_space.n

Â  Â  self.num_actions = env.action_space.n

Â  Â  self.value_table = np.zeros(self.num_states)

Â  Â  self.optimal_policy = np.zeros(self.num_states, dtype=int)

  

Â  def get_optimal_value(self):

Â  Â  for i in range(self.n_iteration):

Â  Â  Â  new_value_table = np.copy(self.value_table)

Â  Â  Â  for state in range(self.num_states):

Â  Â  Â  Â  action_Q = np.zeros(self.num_actions)

Â  Â  Â  Â  for action in range(self.num_actions):

Â  Â  Â  Â  Â  transitions = self.env.P[state][action]

Â  Â  Â  Â  Â  q_sum = 0

Â  Â  Â  Â  Â  for prob, next_state, reward, done in transitions:

Â  Â  Â  Â  Â  Â  q_sum += prob * (reward + self.discount_factor * self.value_table[next_state])

Â  Â  Â  Â  Â  action_Q[action] = q_sum

Â  Â  Â  Â  new_value_table[state] = np.max(action_Q)

Â  Â  Â  self.value_table = new_value_table

  

Â  Â  print("âœ… Optimal Value Table:")

Â  Â  print(self.value_table)

  

Â  def get_optimal_policy(self):

Â  Â  for state in range(self.num_states):

Â  Â  Â  action_Q = np.zeros(self.num_actions)

Â  Â  Â  for action in range(self.num_actions):

Â  Â  Â  Â  transitions = self.env.P[state][action]

Â  Â  Â  Â  q_sum = 0

Â  Â  Â  Â  for prob, next_state, reward, done in transitions:

Â  Â  Â  Â  Â  q_sum += prob * (reward + self.discount_factor * self.value_table[next_state])

Â  Â  Â  Â  action_Q[action] = q_sum

Â  Â  Â  self.optimal_policy[state] = np.argmax(action_Q)

  

Â  Â  print("âœ… Optimal Policy:")

Â  Â  print(self.optimal_policy)

Â  Â  return self.optimal_policy

  
```


``` python


# 5ï¸âƒ£ Apply Value Iteration (unwrap env)


agent = ValueIteration(env.unwrapped, n_iteration=1000, discount_factor=0.9)

agent.get_optimal_value()

policy = agent.get_optimal_policy()

```