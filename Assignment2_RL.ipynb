{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWK9P6Z70EmTUm57HwOUsa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bosy-Ayman/DSAI-402-RL/blob/main/Assignment2_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Assignment: Gridworld MDP Simulation**\n",
        "\n",
        "You are given a rectangular gridworld (like in Figure 3.2 from the main book).\n",
        "Each cell in the grid represents a **state** in the environment.\n",
        "\n",
        "From any state, the agent can take one of four possible **actions**:\n",
        "\n",
        "```\n",
        "NORTH, SOUTH, EAST, WEST\n",
        "```\n",
        "\n",
        "Each action deterministically moves the agent one cell in the chosen direction.\n",
        "\n",
        "If the agent tries to move **off the grid**, it **stays in the same position** and receives a **reward of âˆ’1**.\n",
        "All other moves give a **reward of 0**, except for two special states **A** and **B**:\n",
        "\n",
        "* From **state A = (0, 1)**:\n",
        "  Any action gives a **reward of +10** and moves the agent to **Aâ€² = (4, 1)**.\n",
        "* From **state B = (0, 3)**:\n",
        "  Any action gives a **reward of +5** and moves the agent to **Bâ€² = (2, 3)**.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŽ¯ **Your tasks**\n",
        "\n",
        "1. **Define a policy function Ï€(a|s)**\n",
        "   A function that returns the action to be taken for a given state.\n",
        "\n",
        "2. **Define the environment dynamics**\n",
        "   Implement a function that returns the **next state (sâ€²)** and **reward (r)** given the current state (s) and chosen action (a).\n",
        "   i.e. implement ( p(sâ€²|s,a) ) and ( r(s,a,sâ€²) ).\n",
        "\n",
        "3. **Simulate the Markov chain**\n",
        "\n",
        "   * Choose an initial state (e.g., any grid cell).\n",
        "   * Simulate the process for **1000 time steps** following your policy.\n",
        "   * Repeat this simulation **multiple times** to generate many episodes.\n",
        "\n",
        "4. **Compute the Value Function**\n",
        "   For each state, compute its **value function** ( V(s) ) under **discount rate Î³ = 0.9**,\n",
        "   using the formula:\n",
        "   [\n",
        "   G_t = R_{t+1} + Î³R_{t+2} + Î³^2R_{t+3} + ...\n",
        "   ]\n",
        "\n",
        "5. **Evaluate under two reward schemes:**\n",
        "\n",
        "   #### Reward Scheme 1:\n",
        "\n",
        "   | Condition         | Reward |\n",
        "   | ----------------- | ------ |\n",
        "   | Move off the grid | âˆ’1     |\n",
        "   | Any action from A | +10    |\n",
        "   | Any action from B | +5     |\n",
        "   | All other moves   | 0      |\n",
        "\n",
        "   #### Reward Scheme 2:\n",
        "\n",
        "   | Condition         | Reward |\n",
        "   | ----------------- | ------ |\n",
        "   | Move off the grid | +5     |\n",
        "   | Any action from A | +16    |\n",
        "   | Any action from B | +11    |\n",
        "   | All other moves   | +6     |\n",
        "\n",
        "6. **Compare and Discuss**\n",
        "\n",
        "   * Compute the value function ( V(s) ) for both reward schemes.\n",
        "   * Compare how the value of each state changes under the two settings.\n",
        "   * Discuss why and how the different reward structures affect the agentâ€™s expected return.\n"
      ],
      "metadata": {
        "id": "jRYVR4OVdP5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "C_4y9pmoe1RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MDP Definition Constants\n"
      ],
      "metadata": {
        "id": "trJHZPDCe6qK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "GRID_SIZE = 5\n",
        "GAMMA = 0.9\n",
        "ACTIONS = ['NORTH', 'SOUTH', 'EAST', 'WEST']\n",
        "\n",
        "STATE_A = (0, 1)\n",
        "STATE_B = (0, 3)\n",
        "STATE_A_PRIME = (4, 1)\n",
        "STATE_B_PRIME = (2, 3)"
      ],
      "metadata": {
        "id": "tFwCaX2ce3V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define two reward schemes\n"
      ],
      "metadata": {
        "id": "ptXaGekcfDb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "REWARD_SCHEME_1 = {\n",
        "    'name': 'Standard Rewards',\n",
        "    'OFF_GRID': -1,\n",
        "    'A_REWARD': 10,\n",
        "    'B_REWARD': 5,\n",
        "    'NORMAL_REWARD': 0\n",
        "}\n",
        "\n",
        "REWARD_SCHEME_2 = {\n",
        "    'name': 'High Rewards',\n",
        "    'OFF_GRID': 5,\n",
        "    'A_REWARD': 16,\n",
        "    'B_REWARD': 11,\n",
        "    'NORMAL_REWARD': 6\n",
        "}"
      ],
      "metadata": {
        "id": "vxPaG_zBfBLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION_MAP = {\n",
        "    'NORTH': (-1, 0),\n",
        "    'SOUTH': (1, 0),\n",
        "    'EAST': (0, 1),\n",
        "    'WEST': (0, -1)\n",
        "}"
      ],
      "metadata": {
        "id": "T6XS06e3gKH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Function\n",
        "\n",
        "From any state, choose any action randomly."
      ],
      "metadata": {
        "id": "W9I1duEfgRqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_action(state):\n",
        "    return random.choice(ACTIONS)"
      ],
      "metadata": {
        "id": "-9tnbfKpgNHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transition and Reward Function\n"
      ],
      "metadata": {
        "id": "yYTLLKnOgXHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_state_reward(state, action, reward_scheme):\n",
        "\n",
        "    row, column = state\n",
        "\n",
        "    # if we in grid A --> Move directly to A'\n",
        "    if state == STATE_A:\n",
        "        return STATE_A_PRIME, reward_scheme['A_REWARD']\n",
        "\n",
        "    # if we in grid B --> Move directly to B'\n",
        "    if state == STATE_B:\n",
        "        return STATE_B_PRIME, reward_scheme['B_REWARD']\n",
        "    dr, dc = ACTION_MAP[action]\n",
        "    next_r, next_c = row + dr, column + dc\n",
        "\n",
        "    # Check for moving off the grid\n",
        "    if not (0 <= next_r < GRID_SIZE and 0 <= next_c < GRID_SIZE):\n",
        "        return state, reward_scheme['OFF_GRID']\n",
        "    else:\n",
        "        return (next_r, next_c), reward_scheme['NORMAL_REWARD']"
      ],
      "metadata": {
        "id": "MrrqmLbfgcLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Simulation and Monte Carlo Functions\n"
      ],
      "metadata": {
        "id": "77Vl7LzWgg1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_episode(start_state, max_steps, reward_scheme, gamma):\n",
        "    state = start_state\n",
        "    episode = []\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "\n",
        "        action = get_action(state)\n",
        "\n",
        "        next_state, reward = get_next_state_reward(state, action, reward_scheme)\n",
        "\n",
        "        # Store the transition (s, R)\n",
        "        episode.append({'state': state, 'reward': reward})\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "    return episode"
      ],
      "metadata": {
        "id": "JVLyHrvJgfgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def monte_carlo(num_episodes, max_steps, start_state, reward_scheme, gamma):\n",
        "\n",
        "    # { (row, col): [list of G values for that state] }\n",
        "    returns = {}\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        episode = generate_episode(start_state, max_steps, reward_scheme, gamma)\n",
        "        G = 0\n",
        "        visited_states = set()\n",
        "\n",
        "\n",
        "        for i in reversed(range(len(episode))):\n",
        "            step = episode[i]\n",
        "            s = step['state']\n",
        "            R = step['reward']\n",
        "\n",
        "            G = R + gamma * G\n",
        "\n",
        "            if s not in visited_states:\n",
        "                if s not in returns:\n",
        "                    returns[s] = []\n",
        "\n",
        "                returns[s].append(G)\n",
        "                visited_states.add(s)\n",
        "\n",
        "    # the average of all recorded returns for state s\n",
        "    V_pi = np.zeros((GRID_SIZE, GRID_SIZE))\n",
        "\n",
        "    for r in range(GRID_SIZE):\n",
        "        for c in range(GRID_SIZE):\n",
        "            state = (r, c)\n",
        "            if state in returns and returns[state]:\n",
        "                V_pi[r, c] = np.mean(returns[state])\n",
        "\n",
        "    return V_pi, reward_scheme['name']"
      ],
      "metadata": {
        "id": "z7KPT9_ygmiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_value_function(V_pi, title):\n",
        "    print(f\"\\n Value Function for {title}\")\n",
        "    V_str = \"\"\n",
        "    for r in range(GRID_SIZE):\n",
        "        row_str = \"\"\n",
        "        for c in range(GRID_SIZE):\n",
        "            if (r, c) == STATE_A:\n",
        "                val = f\"{V_pi[r, c]:>5.2f} (A)\"\n",
        "            elif (r, c) == STATE_B:\n",
        "                val = f\"{V_pi[r, c]:>5.2f} (B)\"\n",
        "            else:\n",
        "                val = f\"{V_pi[r, c]:>5.2f}\"\n",
        "            row_str += val + \" | \"\n",
        "        V_str += row_str.strip(' | ') + \"\\n\"\n",
        "        if r < GRID_SIZE - 1:\n",
        "            V_str += \"------\" * GRID_SIZE + \"\\n\"\n",
        "    print(V_str)\n"
      ],
      "metadata": {
        "id": "JppdxaasgqH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define simulation parameters\n"
      ],
      "metadata": {
        "id": "aOIdwmyxhUmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPISODES = 100000\n",
        "MAX_STEPS = 1000\n",
        "START_STATE = (2, 2)"
      ],
      "metadata": {
        "id": "jkHs9j85g9Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulate with Reward Set 1"
      ],
      "metadata": {
        "id": "hiozD_dJheuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V_pi_1, name_1 = monte_carlo(NUM_EPISODES, MAX_STEPS, START_STATE, REWARD_SCHEME_1, GAMMA)\n",
        "print_value_function(V_pi_1, name_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vMOzuWOhDW7",
        "outputId": "5f0260a7-f8b5-45a2-f3c5-beb2973813d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Value Function for Standard Rewards\n",
            "5.25 |  8.40 (A) |  4.90 |  4.62 (B) |  3.00\n",
            "------------------------------\n",
            "2.59 |  3.08 |  2.19 |  1.50 |  1.29\n",
            "------------------------------\n",
            "0.75 |  0.69 |  0.46 | -0.19 |  0.18\n",
            "------------------------------\n",
            "-0.20 | -0.39 | -0.32 | -0.31 | -0.24\n",
            "------------------------------\n",
            "-0.52 | -0.87 | -0.63 | -0.51 | -0.44\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulate with Reward Set 2\n"
      ],
      "metadata": {
        "id": "xSEEQbXihiAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "V_pi_2, name_2 = monte_carlo(NUM_EPISODES, MAX_STEPS, START_STATE, REWARD_SCHEME_2, GAMMA)\n",
        "print_value_function(V_pi_2, name_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jytp8x0mhB4G",
        "outputId": "8b5e7a3c-5c28-4ce8-9381-8c476ecfb064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Value Function for High Rewards\n",
            "62.09 | 62.74 (A) | 61.69 | 58.78 (B) | 59.44\n",
            "------------------------------\n",
            "55.64 | 54.15 | 52.73 | 51.79 | 53.52\n",
            "------------------------------\n",
            "50.10 | 46.82 | 45.21 | 43.15 | 48.40\n",
            "------------------------------\n",
            "46.49 | 41.95 | 41.63 | 42.42 | 46.83\n",
            "------------------------------\n",
            "47.12 | 40.80 | 43.06 | 45.21 | 49.07\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Comparison\n"
      ],
      "metadata": {
        "id": "GnOZqTL1hmm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V_diff = V_pi_2 - V_pi_1\n",
        "\n",
        "constant_shift_r = 6\n",
        "expected_uniform_increase = constant_shift_r / (1 - GAMMA)\n",
        "\n",
        "print(f\"The constant reward difference is {constant_shift_r}. With Î³ = {GAMMA}\")\n",
        "print(f\"The expected uniform increase in V(s) is {expected_uniform_increase:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFUlGwmfd20h",
        "outputId": "53141cb9-fbf4-4693-fd49-687f3116f1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The constant reward difference is 6. With Î³ = 0.9\n",
            "The expected uniform increase in V(s) is 60.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "V_diff_str = \"\"\n",
        "for r in range(GRID_SIZE):\n",
        "    row_str = \"\"\n",
        "    for c in range(GRID_SIZE):\n",
        "        row_str += f\"{V_diff[r, c]:>+6.2f} | \"\n",
        "    V_diff_str += row_str.strip(' | ') + \"\\n\"\n",
        "    if r < GRID_SIZE - 1:\n",
        "        V_diff_str += \"------\" * GRID_SIZE + \"\\n\"\n",
        "print(V_diff_str)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSaoTG61JbIg",
        "outputId": "bc145d48-c79a-460d-b898-0a58cf71a678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+56.84 | +54.33 | +56.79 | +54.15 | +56.44\n",
            "------------------------------\n",
            "+53.04 | +51.07 | +50.54 | +50.29 | +52.23\n",
            "------------------------------\n",
            "+49.35 | +46.14 | +44.75 | +43.34 | +48.23\n",
            "------------------------------\n",
            "+46.69 | +42.33 | +41.95 | +42.73 | +47.07\n",
            "------------------------------\n",
            "+47.64 | +41.67 | +43.69 | +45.72 | +49.51\n",
            "\n"
          ]
        }
      ]
    }
  ]
}